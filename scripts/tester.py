import json
import evaluate
from collections import defaultdict
from pathlib import Path
from typing import Any, Dict, List, Tuple

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import spacy

class NERTester:
    def __init__(self, model_path: str):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞ —Å –∑–∞–≥—Ä—É–∑–∫–æ–π –º–æ–¥–µ–ª–∏
        
        Args:
            model_path: –ø—É—Ç—å –∫ –æ–±—É—á–µ–Ω–Ω–æ–π spaCy –º–æ–¥–µ–ª–∏
        """
        self.model = spacy.load(model_path)
        self.seqeval = evaluate.load("seqeval")
        
    def load_dataset(self, file_path: str, format_type: str = "auto") -> List[Dict[str, Any]]:
        """
        –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞ –∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–æ–≤
        
        Args:
            file_path: –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å –¥–∞–Ω–Ω—ã–º–∏
            format_type: auto, jsonl, json, csv, parquet
            
        Returns:
            –°–ø–∏—Å–æ–∫ —Å–ª–æ–≤–∞—Ä–µ–π —Å –∫–ª—é—á–∞–º–∏ 'text' –∏ 'entities'
        """
        file_path = Path(file_path)
        
        if format_type == "auto":
            # –ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ñ–æ—Ä–º–∞—Ç–∞ –ø–æ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—é
            if file_path.suffix == '.jsonl':
                format_type = 'jsonl'
            elif file_path.suffix == '.json':
                format_type = 'json'
            elif file_path.suffix == '.csv':
                format_type = 'csv'
            elif file_path.suffix == '.parquet':
                format_type = 'parquet'
            else:
                raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç —Ñ–∞–π–ª–∞: {file_path.suffix}")
        
        data = []
        
        if format_type == 'jsonl':
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    if line.strip():
                        data.append(json.loads(line))
                        
        elif format_type == 'json':
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
        elif format_type == 'csv':
            df = pd.read_csv(file_path)
            for _, row in df.iterrows():
                item = {'text': row['text']}
                if 'entities' in row and pd.notna(row['entities']):
                    if isinstance(row['entities'], str):
                        item['entities'] = eval(row['entities'])
                    else:
                        item['entities'] = row['entities']
                data.append(item)
                
        elif format_type == 'parquet':
            df = pd.read_parquet(file_path)
            for _, row in df.iterrows():
                item = {'text': row['text']}
                if 'entities' in row and pd.notna(row['entities']):
                    if isinstance(row['entities'], str):
                        item['entities'] = eval(row['entities'])
                    else:
                        item['entities'] = row['entities']
                data.append(item)
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        validated_data = []
        for item in data:
            if 'text' in item and 'entities' in item:
                validated_data.append({
                    'text': item['text'],
                    'entities': item['entities']
                })
        
        print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(validated_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
        return validated_data
    
    def calculate_iou(self, span1: Tuple[int, int], span2: Tuple[int, int]) -> float:
        """
        –í—ã—á–∏—Å–ª–µ–Ω–∏–µ Intersection over Union –¥–ª—è –¥–≤—É—Ö –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–≤
        
        Args:
            span1: (start, end)
            span2: (start, end)
            
        Returns:
            IoU –∑–Ω–∞—á–µ–Ω–∏–µ –æ—Ç 0 –¥–æ 1
        """
        start1, end1 = span1
        start2, end2 = span2
        
        # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ
        intersection_start = max(start1, start2)
        intersection_end = min(end1, end2)
        intersection = max(0, intersection_end - intersection_start)
        
        # –ù–∞—Ö–æ–¥–∏–º –æ–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ
        union = (end1 - start1) + (end2 - start2) - intersection
        
        return intersection / union if union > 0 else 0.0
    
    def evaluate_entities(self, true_entities: List[Tuple[int, int, str]], 
                        pred_entities: List[Tuple[int, int, str]], 
                        text: str) -> Dict[str, Any]:
        """
        –û—Ü–µ–Ω–∫–∞ —Å—É—â–Ω–æ—Å—Ç–µ–π –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º –¥–ª—è –æ–¥–Ω–æ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
        
        Args:
            true_entities: –∏—Å—Ç–∏–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ [(start, end, label)]
            pred_entities: –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏ [(start, end, label)]
            text: –∏—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏ spans
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ –≤—Å–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
        """
        # –í–∞–ª–∏–¥–∞—Ü–∏—è spans
        valid_true_entities = []
        for start, end, label in true_entities:
            if 0 <= start <= end <= len(text):
                valid_true_entities.append((start, end, label))
        
        valid_pred_entities = []
        for start, end, label in pred_entities:
            if 0 <= start <= end <= len(text):
                valid_pred_entities.append((start, end, label))
        
        # –ü–æ–¥—Å—á–µ—Ç TN - –æ–±—â–∞—è –¥–ª–∏–Ω–∞ —Ç–µ–∫—Å—Ç–∞ –º–∏–Ω—É—Å –≤—Å–µ —Å—É—â–Ω–æ—Å—Ç–∏
        total_chars = len(text)
        total_entity_chars = sum(end - start for start, end, label in valid_true_entities)
        tn_chars = total_chars - total_entity_chars
        
        results = {
            'exact': {'tp': 0, 'tn': tn_chars, 'fp': 0, 'fn': 0},
            'partial': {'tp': 0, 'tn': tn_chars, 'fp': 0, 'fn': 0},
            'type_only': {'tp': 0, 'tn': tn_chars, 'fp': 0, 'fn': 0},
            'bounds_only': {'tp': 0, 'tn': tn_chars, 'fp': 0, 'fn': 0},
            'partial_iou': {'tp': 0, 'tn': tn_chars, 'fp': 0, 'fn': 0}
        }
        
        # –ú–∞—Ç—Ä–∏—Ü—ã –¥–ª—è –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏—è —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–π
        true_matched = {i: False for i in range(len(valid_true_entities))}
        pred_matched = {i: False for i in range(len(valid_pred_entities))}
        
        # –°–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π
        for i, (t_start, t_end, t_label) in enumerate(valid_true_entities):
            for j, (p_start, p_end, p_label) in enumerate(valid_pred_entities):
                # –¢–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ
                if t_start == p_start and t_end == p_end and t_label == p_label:
                    results['exact']['tp'] += 1
                    true_matched[i] = True
                    pred_matched[j] = True
                
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ (–ª—é–±–æ–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ + —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞)
                intersection = max(0, min(t_end, p_end) - max(t_start, p_start))
                if intersection > 0 and t_label == p_label:
                    results['partial']['tp'] += 1
                
                # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ —Ç–∏–ø—É
                if t_label == p_label:
                    results['type_only']['tp'] += 1
                
                # –°–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ –≥—Ä–∞–Ω–∏—Ü–∞–º
                if t_start == p_start and t_end == p_end:
                    results['bounds_only']['tp'] += 1
                
                # –ß–∞—Å—Ç–∏—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –ø–æ IoU > 0.5
                iou = self.calculate_iou((t_start, t_end), (p_start, p_end))
                if iou > 0.5 and t_label == p_label:
                    results['partial_iou']['tp'] += 1
        
        # –ü–æ–¥—Å—á–µ—Ç FP –∏ FN –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
        results['exact']['fn'] = sum(not matched for matched in true_matched.values())
        results['exact']['fp'] = sum(not matched for matched in pred_matched.values())
        
        # –î–ª—è –¥—Ä—É–≥–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π FP/FN —Å—á–∏—Ç–∞—é—Ç—Å—è –ø–æ-—Ä–∞–∑–Ω–æ–º—É
        # (—É–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ - –≤ —Ä–µ–∞–ª—å–Ω–æ–π —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –Ω—É–∂–Ω–æ —É—Ç–æ—á–Ω–∏—Ç—å)
        
        return results
    
    def calculate_metrics(self, tp: int, fp: int, fn: int) -> Dict[str, float]:  
        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0
        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0
        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0.0
        accuracy = tp / (tp + fp + fn) if (tp + fp + fn) > 0 else 0.0  # –±–µ–∑ tn
    
        return {
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'accuracy': accuracy,
            'tp': tp,
            'fp': fp,
            'fn': fn
    }
    
    def evaluate(self, test_data_path: str, output_json_path: str = None, 
                format_type: str = "auto") -> Dict[str, Any]:
        """
        –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–∏
        
        Args:
            test_data_path: –ø—É—Ç—å –∫ —Ç–µ—Å—Ç–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º
            output_json_path: –ø—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            format_type: —Ñ–æ—Ä–º–∞—Ç –¥–∞–Ω–Ω—ã—Ö
            
        Returns:
            –ü–æ–ª–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        test_data = self.load_dataset(test_data_path, format_type)
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—á–µ—Ç—á–∏–∫–æ–≤
        strategy_results = {
            'exact': {'tp': 0,'tn':0,  'fp': 0, 'fn': 0},
            'partial': {'tp': 0,'tn':0, 'fp': 0, 'fn': 0},
            'type_only': {'tp': 0,'tn':0, 'fp': 0, 'fn': 0},
            'bounds_only': {'tp': 0,'tn':0, 'fp': 0, 'fn': 0},
            'partial_iou': {'tp': 0,'tn':0, 'fp': 0, 'fn': 0}
        }
        
        confusion_data = []
        
        print("–ù–∞—á–∏–Ω–∞–µ–º –æ—Ü–µ–Ω–∫—É –º–æ–¥–µ–ª–∏...")
        
        for i, item in enumerate(test_data):
            if i % 100 == 0:
                print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i}/{len(test_data)} –ø—Ä–∏–º–µ—Ä–æ–≤")
            
            text = item['text']
            true_entities = item['entities']
            
            # –ü–æ–ª—É—á–∞–µ–º –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –º–æ–¥–µ–ª–∏
            doc = self.model(text)
            pred_entities = [(ent.start_char, ent.end_char, ent.label_) for ent in doc.ents]
            
            # –û—Ü–µ–Ω–∫–∞ –¥–ª—è —Ç–µ–∫—É—â–µ–≥–æ –ø—Ä–∏–º–µ—Ä–∞
            example_results = self.evaluate_entities(true_entities, pred_entities, text)
            
            # –ê–≥—Ä–µ–≥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º
            for strategy in strategy_results:
                for metric in ['tp', 'tn', 'fp', 'fn']:
                    strategy_results[strategy][metric] += example_results[strategy][metric]
            
            # –°–æ–±–∏—Ä–∞–µ–º –¥–∞–Ω–Ω—ã–µ –¥–ª—è confusion matrix
            for t_start, t_end, t_label in true_entities:
                for p_start, p_end, p_label in pred_entities:
                    if t_start == p_start and t_end == p_end:
                        confusion_data.append((t_label, p_label))
                        
        total_tn_chars = 0
        total_text_chars = 0
    
        for i, item in enumerate(test_data):
            text = item['text']
            true_entities = item['entities']
        
        # –ü–æ–¥—Å—á–µ—Ç TN –¥–ª—è —ç—Ç–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
            total_text_chars += len(text)
            total_entity_chars = sum(end - start for start, end, label in true_entities)
            total_tn_chars += len(text) - total_entity_chars
        
        # –†–∞—Å—á–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        final_results = {}
        
        for strategy, counts in strategy_results.items():
            final_results[strategy] = self.calculate_metrics(
                counts['tp'], counts['fp'], counts['fn']
            )
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è confusion matrix
        confusion_matrix = self._create_confusion_matrix(confusion_data)
        
        # –°–±–æ—Ä–∫–∞ –ø–æ–ª–Ω—ã—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        full_results = {
            'overall_metrics': final_results,
            'tn_stats': {
                    'total_tn_chars': total_tn_chars,
                    'total_text_chars': total_text_chars,
                    'tn_ratio': total_tn_chars / total_text_chars if total_text_chars > 0 else 0.0},
            'confusion_matrix': confusion_matrix,
            'test_info': {
                'dataset_size': len(test_data),
                'model_path': str(self.model.path if hasattr(self.model, 'path') else 'unknown'),
                'evaluation_date': pd.Timestamp.now().isoformat()}
        }
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        if output_json_path:
            with open(output_json_path, 'w', encoding='utf-8') as f:
                json.dump(full_results, f, indent=2, ensure_ascii=False)
            print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {output_json_path}")
        
        # –í—ã–≤–æ–¥ –≤ –∫–æ–Ω—Å–æ–ª—å
        self._print_report(full_results)
        
        return full_results
    
    def _create_confusion_matrix(self, confusion_data: List[Tuple[str, str]]) -> Dict[str, Any]:
        """–°–æ–∑–¥–∞–Ω–∏–µ confusion matrix –¥–∞–Ω–Ω—ã—Ö"""
        if not confusion_data:
            return {}
            
        true_labels, pred_labels = zip(*confusion_data)
        
        # –í—Å–µ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –º–µ—Ç–∫–∏
        all_labels = sorted(set(true_labels) | set(pred_labels))
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–∞—Ç—Ä–∏—Ü—ã
        matrix = np.zeros((len(all_labels), len(all_labels)), dtype=int)
        label_to_idx = {label: idx for idx, label in enumerate(all_labels)}
        
        for true_label, pred_label in confusion_data:
            true_idx = label_to_idx[true_label]
            pred_idx = label_to_idx[pred_label]
            matrix[true_idx][pred_idx] += 1
        
        return {
            'matrix': matrix.tolist(),
            'labels': all_labels,
            'label_to_idx': label_to_idx
        }
    
    def _print_report(self, results: Dict[str, Any]):
        """–ö—Ä–∞—Å–∏–≤—ã–π –≤—ã–≤–æ–¥ –æ—Ç—á–µ—Ç–∞ –≤ –∫–æ–Ω—Å–æ–ª—å"""
        print("\n" + "="*80)
        print("–û–¢–ß–ï–¢ –ü–û –û–¶–ï–ù–ö–ï NER –ú–û–î–ï–õ–ò")
        print("="*80)
        
        overall = results['overall_metrics']
        
        for strategy, metrics in overall.items():
            print(f"\n--- {strategy.upper()} ---")
            print(f"Precision:  {metrics['precision']:.4f}")
            print(f"Recall:     {metrics['recall']:.4f}")
            print(f"F1-Score:   {metrics['f1']:.4f}")
            print(f"Accuracy: {metrics['accuracy']:.4f}")
            print(f"TP/FP/FN: {metrics['tp']}/{metrics['fp']}/{metrics['fn']}")            
            print(f"TN: {results['tn_stats']['total_tn_chars']}")
            print("\nüìä –ü–û–Ø–°–ù–ï–ù–ò–ï –ú–ï–¢–†–ò–ö:")
            print("TP - –ü—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞–π–¥–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏\nFP - –õ–æ–∂–Ω—ã–µ —Å—Ä–∞–±–∞—Ç—ã–≤–∞–Ω–∏—è (–Ω–∞–π–¥–µ–Ω–æ –ª–∏—à–Ω–µ–µ)\nFN - –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ —Å—É—â–Ω–æ—Å—Ç–∏\nTN - –°–∏–º–≤–æ–ª—ã –±–µ–∑ —Å—É—â–Ω–æ—Å—Ç–µ–π (–Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –≤ —Ä–∞—Å—á–µ—Ç–∞—Ö)")

            if strategy.upper() == "EXACT":
                print("EXACT - —Å—Ç—Ä–æ–≥–∞—è –æ—Ü–µ–Ω–∫–∞, —Ç–æ—á–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –≥—Ä–∞–Ω–∏—Ü –∏ —Ç–∏–ø–æ–≤")
            if strategy.upper() == "PARTIAL":
                print("PARTIAL - —É—á–∏—Ç—ã–≤–∞–µ—Ç –Ω–µ–±–æ–ª—å—à–∏–µ –æ—à–∏–±–∫–∏ –≤ –≥—Ä–∞–Ω–∏—Ü–∞—Ö")
            if strategy.upper() == "TYPE_ONLY":
                print("TYPE_ONLY - –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—é —Ç–∏–ø–æ–≤")
            if strategy.upper() == "BOUNDS_ONLY":
                print(
                    "BOUNDS_ONLY - –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Ç–æ–ª—å–∫–æ –¥–µ—Ç–µ–∫—Ü–∏—é –≥—Ä–∞–Ω–∏—Ü")
            if strategy.upper() == "PARTIAL_IOU":
                print("Intersection-over-Union - –ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ —Å—É—â–Ω–æ—Å—Ç–µ–π > 50% + —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ —Ç–∏–ø–∞ (–ü–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ) / (–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ)")
        # –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è confusion matrix
        self._plot_confusion_matrix(results['confusion_matrix'])
    
    def _plot_confusion_matrix(self, confusion_data: Dict[str, Any]):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è confusion matrix"""
        if not confusion_data or not confusion_data.get('matrix'):
            print("\n–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è confusion matrix")
            return
            
        matrix = np.array(confusion_data['matrix'])
        labels = confusion_data['labels']
        
        plt.figure(figsize=(12, 10))
        sns.heatmap(matrix, annot=True, fmt='d', cmap='Blues', 
                    xticklabels=labels, yticklabels=labels)
        plt.title('Confusion Matrix')
        plt.xlabel('Predicted Labels')
        plt.ylabel('True Labels')
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)
        plt.tight_layout()
        plt.show()


# –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç–µ—Å—Ç–µ—Ä–∞
    tester = NERTester("../the-coolest/models/best_model")
    
    # –ó–∞–ø—É—Å–∫ –æ—Ü–µ–Ω–∫–∏
    results = tester.evaluate(
        test_data_path="../the-coolest/data/val_data.json",
        output_json_path="evaluation_results.json",
        format_type="json"
    )
    
#(config_path="../the-coolest/models/best_model/config.cfg",